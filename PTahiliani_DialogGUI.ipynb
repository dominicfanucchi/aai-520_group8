{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load DialoGPT-small model and tokenizer\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available and move model to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set chat parameters\n",
    "max_length = 1000\n",
    "chat_history_ids = None\n",
    "\n",
    "def chat_with_bot(user_input):\n",
    "    global chat_history_ids\n",
    "    \n",
    "    # Encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Append the new user input tokens to the chat history\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_user_input_ids\n",
    "\n",
    "    # Generate a response while limiting the total chat history to 1000 tokens\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, \n",
    "        max_length=max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.7,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # Extract the AI's response\n",
    "    ai_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    # Optionally, trim conversation history if it gets too long\n",
    "    if chat_history_ids.shape[1] > 1000:\n",
    "        chat_history_ids = chat_history_ids[:, -1000:]\n",
    "    \n",
    "    return ai_response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "october and november\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load DialoGPT-small model and tokenizer\n",
    "model_name = \"my_awesome_qa_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available and move model to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set chat parameters\n",
    "max_length = 1000\n",
    "chat_history_ids = None\n",
    "\n",
    "def answer_question(question, context):\n",
    "    # Encode the question and context\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the start and end scores\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Get the most likely start and end of the answer\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores)\n",
    "\n",
    "    # Convert token indices to the answer string\n",
    "    answer_tokens = inputs['input_ids'][0][start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = \"Thanksgiving is a national holiday celebrated on various dates in October and November in the United States, Canada, Saint Lucia, Liberia, and unofficially in countries like Brazil, Germany and the Philippines. It is also observed in the Dutch town of Leiden and the Australian territory of Norfolk Island. It began as a day of giving thanks for the blessings of the harvest and of the preceding year. Various similarly named harvest festival holidays occur throughout the world during autumn. Although Thanksgiving has historical roots in religious and cultural traditions, it has long been celebrated as a secular holiday as well.\"\n",
    "user_question = \"When is thanksgiving celebrated?\"\n",
    "response = answer_question(user_question, context)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "\n",
    "user_chat_history = []\n",
    "\n",
    "def update_gui_chat(*args):\n",
    "    global user_chat_history\n",
    "    \n",
    "    user_input = dialog.get()  # Get the input from the user\n",
    "    \n",
    "    # Check for termination commands\n",
    "    if user_input.lower() in [\"quit\", \"bye\"]:\n",
    "        chat.set(\"Thank you for chatting! Goodbye!\")\n",
    "        dialog.set(\"\")  # Clear the entry box\n",
    "        return  # Exit the function early\n",
    "    \n",
    "    # Append the user input to the chat history\n",
    "    user_chat_history.append(\"User: \" + user_input)\n",
    "    \n",
    "    # Get the AI's response from the chat_with_bot function\n",
    "    ai_response = chat_with_bot(user_input)\n",
    "    \n",
    "    # Append the AI's response to the chat history\n",
    "    user_chat_history.append(\"AI: \" + ai_response)\n",
    "\n",
    "    # Update the chat_history StringVar to display in the UI\n",
    "    chat.set(\"\\n\".join(user_chat_history))\n",
    "    \n",
    "    # Clear the input field after sending the message\n",
    "    dialog.set(\"\")  # Clear the entry box\n",
    "\n",
    "# Create the main application window\n",
    "app = Tk()\n",
    "app.title(\"AI Dialog Chat\")\n",
    "\n",
    "mainframe = ttk.Frame(app, padding=\"50 3 20 12\")\n",
    "mainframe.grid(column=0, row=0, sticky=(N, W, E, S))\n",
    "\n",
    "# Configure the grid to have 5 columns and 5 rows\n",
    "for i in range(5):\n",
    "    mainframe.columnconfigure(i, weight=1)  # Allow columns to expand\n",
    "    mainframe.rowconfigure(i, weight=1)     # Allow rows to expand\n",
    "\n",
    "# Row 1: Instructions\n",
    "label = ttk.Label(mainframe, text=\"Chat with the AI. Type 'exit', 'quit', or 'bye' to finish the conversation.\")\n",
    "label.grid(column=2, row=1, sticky=(W, E))\n",
    "\n",
    "# Row 2: Chat History\n",
    "label = ttk.Label(mainframe, text=\"Chat History:\")\n",
    "label.grid(column=1, row=2, sticky=(W, E))\n",
    "chat = StringVar()\n",
    "ttk.Label(mainframe, textvariable=chat).grid(column=2, row=2, sticky=(W, E))\n",
    "\n",
    "# Row 3: Input Box\n",
    "label = ttk.Label(mainframe, text=\"Input: \")\n",
    "label.grid(column=1, row=3, sticky=(W, E))\n",
    "\n",
    "dialog = StringVar()\n",
    "dialog_entry = ttk.Entry(mainframe, width=150, textvariable=dialog)\n",
    "dialog_entry.grid(column=2, row=3, sticky=(W, E))\n",
    "\n",
    "# Chat button to trigger the update_gui_chat function\n",
    "ttk.Button(mainframe, text=\"Chat\", command=update_gui_chat).grid(column=2, row=4, sticky=(W, E))\n",
    "\n",
    "# Add padding to all child widgets\n",
    "for child in mainframe.winfo_children(): \n",
    "    child.grid_configure(padx=5, pady=5)\n",
    "\n",
    "# Set focus to the entry field and bind the Enter key to the chat function\n",
    "dialog_entry.focus()\n",
    "app.bind(\"<Return>\", update_gui_chat)\n",
    "\n",
    "# Run the application\n",
    "app.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    bot_message = chat_with_bot(message)\n",
    "    chat_history.append((message, bot_message))\n",
    "    \n",
    "    # Optionally limit chat history to a certain length\n",
    "    if len(chat_history) > 10:  # Keep only the last 10 exchanges\n",
    "        chat_history = chat_history[-10:]\n",
    "\n",
    "    return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Function to respond to questions\n",
    "def respond(user_question, context, chat_history):\n",
    "    if context.strip() == \"\":\n",
    "        return \"Please provide context.\", chat_history\n",
    "\n",
    "    answer = answer_question(user_question, context)\n",
    "    chat_history.append((user_question, answer))  # Append user question and bot answer to chat history\n",
    "    return \"\", chat_history\n",
    "\n",
    "# Create Gradio Blocks interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### Question Answering Chat\")\n",
    "    context = gr.Textbox(label=\"Context\", placeholder=\"Enter the context here...\", lines=5)\n",
    "    question = gr.Textbox(label=\"Question\", placeholder=\"Ask your question...\", lines=1)\n",
    "    chatbot = gr.Chatbot()  # Create a chat-like interface\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    # Handle submit button for questions\n",
    "    question.submit(respond, inputs=[question, context, chatbot], outputs=[question, chatbot])\n",
    "    \n",
    "    # Handle clear button to reset chat\n",
    "    clear.click(lambda: (\"\", []), outputs=[context, chatbot])  # Clear context and chat history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAI520",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
