{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pawan\\anaconda3\\envs\\AAI520\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load DialoGPT-small model and tokenizer\n",
    "model_name = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Check if GPU is available and move model to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set chat parameters\n",
    "max_length = 1000\n",
    "chat_history_ids = None\n",
    "\n",
    "def chat_with_bot(tkinter_chat_input):\n",
    "    global chat_history_ids\n",
    "    \n",
    "    # Encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(tkinter_chat_input + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Create the attention mask\n",
    "    new_user_attention_mask = torch.ones(new_user_input_ids.shape, dtype=torch.long, device=device)\n",
    "\n",
    "    # Append the new user input tokens to the chat history\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "        attention_mask = torch.cat([torch.ones(chat_history_ids.shape, dtype=torch.long, device=device), new_user_attention_mask], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_user_input_ids\n",
    "        attention_mask = new_user_attention_mask\n",
    "\n",
    "    # Generate a response while limiting the total chat history to 1000 tokens\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, \n",
    "        max_length=max_length,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True,\n",
    "        top_k=100,\n",
    "        top_p=0.7,\n",
    "        temperature=0.8,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    \n",
    "    # Extract the AI's response\n",
    "    ai_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    # Optionally, trim conversation history if it gets too long\n",
    "    if chat_history_ids.shape[1] > 1000:\n",
    "        chat_history_ids = chat_history_ids[:, -1000:]\n",
    "    \n",
    "    return ai_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pawan\\anaconda3\\envs\\AAI520\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "\n",
    "user_chat_history = []\n",
    "\n",
    "def updatechat(*args):\n",
    "    global user_chat_history\n",
    "    \n",
    "    user_input = dialog.get()    \n",
    "    \n",
    "    # Check for termination commands\n",
    "    if user_input.lower() in [\"quit\", \"bye\"]:\n",
    "        chat.set(\"Thank you for chatting! Goodbye!\")\n",
    "        dialog.set(\"\")  # Clear the entry box\n",
    "        return  # Exit the function early\n",
    "    \n",
    "    user_chat_history.append(user_input)\n",
    "    \n",
    "    if len(user_chat_history) > 1000:\n",
    "        user_chat_history = user_chat_history[-1000:]\n",
    "    \n",
    "    # Get the AI's response\n",
    "    ai_response = chat_with_bot(user_input)\n",
    "    \n",
    "    # Append the AI's response to the chat history\n",
    "    user_chat_history.append(ai_response)\n",
    "\n",
    "    # Update the chat_history StringVar\n",
    "    chat.set(\"\\n\".join(user_chat_history))\n",
    "    \n",
    "    # Clear the entry field after sending the message\n",
    "    dialog.set(\"\")  # Clear the entry box\n",
    "\n",
    "# Create the main application window\n",
    "app = Tk()\n",
    "app.title(\"AAI520 Dialog Chat\")\n",
    "\n",
    "mainframe = ttk.Frame(app, padding=\"50 3 20 12\")\n",
    "mainframe.grid(column=0, row=0, sticky=(N, W, E, S))\n",
    "\n",
    "# Configure the grid to have 5 columns and 5 rows\n",
    "for i in range(5):\n",
    "    mainframe.columnconfigure(i, weight=1)  # Allow columns to expand\n",
    "    mainframe.rowconfigure(i, weight=1)     # Allow rows to expand\n",
    "\n",
    "# Row 1: Instructions\n",
    "label = ttk.Label(mainframe, text=\"Chat with the AI. Type 'exit', 'quit', or 'end' to finish the conversation.\")\n",
    "label.grid(column=2, row=1, sticky=(W, E))\n",
    "\n",
    "# Row 2: Chat History\n",
    "label = ttk.Label(mainframe, text=\"Chat History:\")\n",
    "label.grid(column=1, row=2, sticky=(W, E))\n",
    "chat = StringVar()\n",
    "ttk.Label(mainframe, textvariable=chat).grid(column=2, row=2, sticky=(W, E))\n",
    "\n",
    "# Row 3: Input Box\n",
    "label = ttk.Label(mainframe, text=\"Input: \")\n",
    "label.grid(column=1, row=3, sticky=(W, E))\n",
    "\n",
    "dialog = StringVar()\n",
    "dialog_entry = ttk.Entry(mainframe, width=150, textvariable=dialog)\n",
    "dialog_entry.grid(column=2, row=3, sticky=(W, E))\n",
    "\n",
    "# Chat button\n",
    "ttk.Button(mainframe, text=\"Chat\", command=updatechat).grid(column=2, row=4, sticky=(W, E))\n",
    "\n",
    "# Add padding to all child widgets\n",
    "for child in mainframe.winfo_children(): \n",
    "    child.grid_configure(padx=5, pady=5)\n",
    "\n",
    "# Set focus to the entry field and bind the Enter key to the chat function\n",
    "dialog_entry.focus()\n",
    "app.bind(\"<Return>\", updatechat)\n",
    "\n",
    "# Run the application\n",
    "app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    bot_message = chat_with_bot(message)\n",
    "    chat_history.append((message, bot_message))\n",
    "    \n",
    "    # Optionally limit chat history to a certain length\n",
    "    if len(chat_history) > 10:  # Keep only the last 10 exchanges\n",
    "        chat_history = chat_history[-10:]\n",
    "\n",
    "    return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAI520",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
